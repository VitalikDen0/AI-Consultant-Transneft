#!/usr/bin/env python3
"""
LLM Chat Interface with GGUF Model Support

–ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π GGUF –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ llama-cpp-python.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç thinking mode –∏ –ø–æ–ª–Ω—É—é –≤—ã–≥—Ä—É–∑–∫—É –Ω–∞ GPU.
"""

import os
import sys
import json
import argparse
import time
from typing import Optional, Dict, Any, List
from pathlib import Path

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ CUDA –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

try:
    from llama_cpp import Llama
    from colorama import init, Fore, Style, Back
    import psutil
    from audio_handler import AudioHandler
    from vision_handler import VisionHandler
except ImportError as e:
    print(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: pip install -r requirements.txt")
    sys.exit(1)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è colorama –¥–ª—è Windows
init(autoreset=True)

class LLMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è LLM –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    
    def __init__(self, model_path: Optional[str] = None, config_path: Optional[str] = None):
        # –ò–º–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ config.py
        try:
            import config as app_config
        except ImportError:
            raise ImportError("‚ùå config.py –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª config.py —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.")
        
        # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∞—Ä–≥—É–º–µ–Ω—Ç -> config.py)
        self.model_path = model_path if model_path else app_config.MODEL_PATH
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.n_ctx = app_config.CONTEXT_SIZE
        self.n_gpu_layers = app_config.GPU_LAYERS
        self.main_gpu = app_config.MAIN_GPU
        self.n_batch = app_config.BATCH_SIZE
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–º—è—Ç–∏
        self.use_mmap = app_config.USE_MMAP
        self.use_mlock = app_config.USE_MLOCK
        
        # KV cache –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.type_k = self._parse_kv_type(app_config.KV_CACHE_TYPE_K)
        self.type_v = self._parse_kv_type(app_config.KV_CACHE_TYPE_V)
        
        # CPU –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.n_threads = psutil.cpu_count()
        self.n_threads_batch = psutil.cpu_count()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.temperature = app_config.TEMPERATURE
        self.top_p = app_config.TOP_P
        self.top_k = app_config.TOP_K
        self.min_p = app_config.MIN_P
        self.max_tokens = app_config.MAX_TOKENS
        self.max_thinking_tokens = app_config.MAX_THINKING_TOKENS
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.flash_attn = app_config.FLASH_ATTENTION
        self.rope_scaling_type = app_config.ROPE_SCALING_TYPE
        self.split_mode = app_config.SPLIT_MODE
        self.numa = app_config.NUMA
        
        # –û—Ç–ª–∞–¥–∫–∞
        self.verbose = app_config.VERBOSE_LOGGING
        
        print(f"{Fore.GREEN}‚úì –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ config.py")
    
    def _parse_kv_type(self, type_str: str) -> int:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–æ–≤–æ–≥–æ —Ç–∏–ø–∞ KV –∫—ç—à–∞ –≤ —Ü–∏—Ñ—Ä—É"""
        type_mapping = {
            'q4_0': 2,
            'q4_1': 3,
            'q5_0': 6,
            'q5_1': 7,
            'q8_0': 8,
            'q8_1': 9,
            'f16': 1,
            'f32': 0
        }
        return type_mapping.get(type_str.lower(), 8)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Q8_0
    
    def print_config(self):
        """–í—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        print(f"{Fore.MAGENTA}=== –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===")
        print(f"{Fore.BLUE}–ú–æ–¥–µ–ª—å: {self.model_path}")
        print(f"{Fore.BLUE}–ö–æ–Ω—Ç–µ–∫—Å—Ç: {self.n_ctx}")
        print(f"{Fore.BLUE}GPU —Å–ª–æ–∏: {self.n_gpu_layers}")
        print(f"{Fore.BLUE}–û—Å–Ω–æ–≤–Ω–æ–π GPU: {self.main_gpu}")
        print(f"{Fore.BLUE}Temperature: {self.temperature}")
        print(f"{Fore.BLUE}Top P: {self.top_p}")
        print(f"{Fore.BLUE}Top K: {self.top_k}")
        print(f"{Fore.BLUE}KV cache: type_k={self.type_k}, type_v={self.type_v}")
        print(f"{Fore.BLUE}Flash attention: {self.flash_attn}")
        print(f"{Fore.BLUE}MMAP: {self.use_mmap}, MLOCK: {self.use_mlock}")
        print(f"{Fore.MAGENTA}=========================")

class ThinkingLLM:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å thinking –º–æ–¥–µ–ª—å—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.llama = None
        self.conversation_history = []  # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
        self.max_history_tokens = config.n_ctx // 2  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ (–ø–æ–ª–æ–≤–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ PROMT.md
        self.knowledge_base = self._load_knowledge_base()
        
        self._initialize_model()
    
    def _load_knowledge_base(self) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ PROMT.md"""
        try:
            promt_path = Path(__file__).parent / "PROMT.md"
            if promt_path.exists():
                with open(promt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"{Fore.GREEN}‚úì –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ PROMT.md ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                return content
            else:
                print(f"{Fore.YELLOW}‚ö†Ô∏è  –§–∞–π–ª PROMT.md –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç")
                return ""
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return ""
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        print(f"{Fore.YELLOW}–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
        print(f"{Fore.CYAN}–ú–æ–¥–µ–ª—å: {self.config.model_path}")
        print(f"{Fore.CYAN}–ö–æ–Ω—Ç–µ–∫—Å—Ç: {self.config.n_ctx} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"{Fore.CYAN}GPU —Å–ª–æ–∏: {self.config.n_gpu_layers} (–ø–æ–ª–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞)")
        print(f"{Fore.CYAN}–û—Å–Ω–æ–≤–Ω–æ–π GPU: {self.config.main_gpu}")
        print(f"{Fore.CYAN}CPU –ø–æ—Ç–æ–∫–∏: {self.config.n_threads}")
        print(f"{Fore.CYAN}KV –∫—ç—à: type_k={self.config.type_k}, type_v={self.config.type_v}")
        print(f"{Fore.CYAN}KV –∫—ç—à: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        
        if not Path(self.config.model_path).exists():
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.config.model_path}")
        
        try:
            self.llama = Llama(
                model_path=self.config.model_path,
                n_ctx=self.config.n_ctx,
                n_batch=self.config.n_batch,  # –î–æ–±–∞–≤–ª—è–µ–º batch size
                n_gpu_layers=self.config.n_gpu_layers,
                main_gpu=self.config.main_gpu,
                use_mmap=self.config.use_mmap,  # True –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
                use_mlock=self.config.use_mlock,  # False –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
                n_threads=self.config.n_threads,
                n_threads_batch=self.config.n_threads_batch,
                rope_scaling_type=self.config.rope_scaling_type,
                verbose=self.config.verbose,
                split_mode=self.config.split_mode,
                numa=self.config.numa,
                # VRAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                offload_kqv=True,  # –í—ã–≥—Ä—É–∑–∫–∞ KV –∫—ç—à–∞ –Ω–∞ GPU
                logits_all=False,  # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ logits
                embedding=False,  # –û—Ç–∫–ª—é—á–∞–µ–º embedding –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω
                # KV cache –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
                type_k=self.config.type_k,
                type_v=self.config.type_v,
                flash_attn=self.config.flash_attn
            )
            print(f"{Fore.GREEN}‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            self._print_model_info()
        except Exception as e:
            error_msg = str(e)
            print(f"{Fore.RED}–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {error_msg}")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CUDA –æ—à–∏–±–æ–∫
            if "CUDA" in error_msg or "cuda" in error_msg:
                print(f"{Fore.YELLOW}üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CUDA –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ CPU...")
                try:
                    # Fallback –Ω–∞ CPU
                    self.config.n_gpu_layers = 0
                    self.llama = Llama(
                        model_path=self.config.model_path,
                        n_ctx=self.config.n_ctx,
                        n_gpu_layers=0,  # –¢–æ–ª—å–∫–æ CPU
                        use_mmap=self.config.use_mmap,
                        use_mlock=self.config.use_mlock,
                        n_threads=self.config.n_threads,
                        verbose=self.config.verbose,
                        # KV cache –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–∞–∂–µ –Ω–∞ CPU
                        type_k=self.config.type_k,
                        type_v=self.config.type_v,
                    )
                    print(f"{Fore.GREEN}‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU!")
                    self._print_model_info()
                    return
                except Exception as cpu_error:
                    print(f"{Fore.RED}–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU: {cpu_error}")
            
            raise
    
    def _print_model_info(self):
        """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        if self.llama is None:
            print(f"{Fore.YELLOW}–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return
            
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏
            metadata = getattr(self.llama, 'metadata', {})
            vocab_size = self.llama.n_vocab() if hasattr(self.llama, 'n_vocab') else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            ctx_size = self.llama.n_ctx() if hasattr(self.llama, 'n_ctx') else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            
            print(f"{Fore.MAGENTA}=== –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ ===")
            print(f"{Fore.BLUE}–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
            print(f"{Fore.BLUE}–ö–æ–Ω—Ç–µ–∫—Å—Ç: {ctx_size}")
            
            # –í—ã–≤–æ–¥ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            if metadata and isinstance(metadata, dict):
                for key, value in metadata.items():
                    if key.startswith('general.'):
                        print(f"{Fore.BLUE}{key}: {value}")
            
        except Exception as e:
            print(f"{Fore.YELLOW}–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: {e}")
    
    def _trim_conversation_history(self):
        """
        –û–±—Ä–µ–∑–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if not self.conversation_history or self.llama is None:
            return
        
        # –°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        system_messages = [msg for msg in self.conversation_history if msg['role'] == 'system']
        other_messages = [msg for msg in self.conversation_history if msg['role'] != 'system']
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        total_tokens = 0
        kept_messages = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (—Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ)
        for message in reversed(other_messages):
            try:
                message_text = f"<|im_start|>{message['role']}\n{message['content']}<|im_end|>\n"
                message_tokens = len(self.llama.tokenize(message_text.encode('utf-8')))
                
                if total_tokens + message_tokens <= self.max_history_tokens:
                    kept_messages.insert(0, message)
                    total_tokens += message_tokens
                else:
                    break
            except Exception:
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                estimated_tokens = len(message['content'].split()) * 1.3
                if total_tokens + estimated_tokens <= self.max_history_tokens:
                    kept_messages.insert(0, message)
                    total_tokens += estimated_tokens
                else:
                    break
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history = system_messages + kept_messages
        
        if len(other_messages) > len(kept_messages):
            removed_count = len(other_messages) - len(kept_messages)
            print(f"{Fore.YELLOW}üìù –£–¥–∞–ª–µ–Ω–æ {removed_count} —Å—Ç–∞—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤")
    
    def add_to_history(self, role: str, content: str):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
        """
        self.conversation_history.append({
            "role": role,
            "content": content
        })
        self._trim_conversation_history()
    
    def clear_history(self):
        """
        –û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ (–∫—Ä–æ–º–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è)
        """
        system_messages = [msg for msg in self.conversation_history if msg['role'] == 'system']
        self.conversation_history = system_messages
        print(f"{Fore.GREEN}üóëÔ∏è –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞")
    
    def get_history_summary(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
        """
        if not self.conversation_history:
            return "–ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞"
        
        user_messages = len([msg for msg in self.conversation_history if msg['role'] == 'user'])
        assistant_messages = len([msg for msg in self.conversation_history if msg['role'] == 'assistant'])
        
        return f"–ò—Å—Ç–æ—Ä–∏—è: {user_messages} –≤–æ–ø—Ä–æ—Å–æ–≤, {assistant_messages} –æ—Ç–≤–µ—Ç–æ–≤"

    def _create_chat_template(self, messages: List[Dict[str, str]]) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ chat template —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É Jinja —à–∞–±–ª–æ–Ω—É
        """
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Jinja —à–∞–±–ª–æ–Ω–∞
        # –í –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–µ–¥–æ–≤–∞–ª–æ –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å jinja2
        
        formatted_messages = []
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        if messages and messages[0].get('role') == 'system':
            formatted_messages.append(f"<|im_start|>system\n{messages[0]['content']}<|im_end|>")
            messages = messages[1:]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        for message in messages:
            role = message['role']
            content = message['content']
            
            if role in ['user', 'assistant']:
                formatted_messages.append(f"<|im_start|>{role}\n{content}<|im_end|>")
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        formatted_messages.append("<|im_start|>assistant\n")
        
        return "\n".join(formatted_messages)
    
    def generate_response(self, user_input: str) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å thinking —Ä–µ–∂–∏–º–æ–º –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π —Ç–æ–∫–µ–Ω–æ–≤
        """
        if self.llama is None:
            return {
                "thinking": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
                "answer": "–û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞",
                "full_response": "Model not loaded",
                "token_stats": {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0, "tokens_per_second": 0.0}
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.add_to_history("user", user_input)
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        if not any(msg['role'] == 'system' for msg in self.conversation_history):
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
            system_prompt = "–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π AI-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏ –¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å. "
            system_prompt += "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å. "
            system_prompt += "–û—Ç–≤–µ—á–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
            system_prompt += "–ò—Å–ø–æ–ª—å–∑—É–π <think></think> —Ç–µ–≥–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º.\n\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –µ—Å–ª–∏ –æ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            if self.knowledge_base:
                system_prompt += "# –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –û –ö–û–ú–ü–ê–ù–ò–ò –¢–†–ê–ù–°–ù–ï–§–¢–¨\n\n"
                system_prompt += self.knowledge_base
                system_prompt += "\n\n# –ö–û–ù–ï–¶ –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô\n\n"
                system_prompt += "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–º–ø–∞–Ω–∏–∏. "
                system_prompt += "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ."
            
            self.add_to_history("system", system_prompt)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        messages = self.conversation_history.copy()
        
        prompt = self._create_chat_template(messages)
        
        # –ü–æ–¥—Å—á–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        try:
            input_tokens = len(self.llama.tokenize(prompt.encode('utf-8')))
        except Exception:
            input_tokens = 0
        
        print(f"{Fore.YELLOW}–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        print(f"{Fore.CYAN}üìä –í—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {input_tokens}")
        print(f"{Fore.CYAN}üìö {self.get_history_summary()}")
        
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        start_time = time.time()
        
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º)
            response = self.llama(
                prompt,
                max_tokens=self.config.max_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                top_k=self.config.top_k,
                min_p=self.config.min_p,
                stop=["<|im_end|>", "</s>"],  # –°—Ç–æ–ø —Ç–æ–∫–µ–Ω—ã
                echo=False,  # –ù–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –ø—Ä–æ–º–ø—Ç
                stream=False,  # –ë–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            )
            
            # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
            end_time = time.time()
            generation_time = end_time - start_time
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            if isinstance(response, dict) and 'choices' in response:
                full_response = response['choices'][0]['text'].strip()
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
                usage = response.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', input_tokens)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', prompt_tokens + completion_tokens)
                
                # –ï—Å–ª–∏ completion_tokens –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–∞–º–∏
                if completion_tokens == 0 and full_response:
                    try:
                        completion_tokens = len(self.llama.tokenize(full_response.encode('utf-8')))
                        total_tokens = prompt_tokens + completion_tokens
                    except Exception:
                        completion_tokens = len(full_response.split())  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                        total_tokens = prompt_tokens + completion_tokens
                
            else:
                # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
                full_response = str(response).strip()
                prompt_tokens = input_tokens
                try:
                    completion_tokens = len(self.llama.tokenize(full_response.encode('utf-8')))
                except Exception:
                    completion_tokens = len(full_response.split())  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                total_tokens = prompt_tokens + completion_tokens
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            tokens_per_second = completion_tokens / generation_time if generation_time > 0 else 0.0
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ thinking –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            thinking_content = ""
            final_answer = full_response
            
            if "<think>" in full_response and "</think>" in full_response:
                parts = full_response.split("<think>", 1)
                if len(parts) > 1:
                    think_part = parts[1].split("</think>", 1)
                    if len(think_part) > 1:
                        thinking_content = think_part[0].strip()
                        final_answer = think_part[1].strip()
            
            # –ï—Å–ª–∏ thinking –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º
            if not thinking_content:
                thinking_content = "Reasoning process not found in response"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é (—Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –±–µ–∑ thinking)
            self.add_to_history("assistant", final_answer)
            
            # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
            print(f"\n{Fore.CYAN}üí≠ Thinking:{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}{thinking_content}{Style.RESET_ALL}")
            print(f"\n{Fore.GREEN}‚úÖ Answer:{Style.RESET_ALL}")
            print(f"{Fore.WHITE}{final_answer}{Style.RESET_ALL}\n")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
            token_stats = {
                "input_tokens": prompt_tokens,
                "output_tokens": completion_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "generation_time": generation_time
            }
            
            return {
                "thinking": thinking_content,
                "answer": final_answer,
                "full_response": full_response,
                "token_stats": token_stats
            }
            
        except Exception as e:
            end_time = time.time()
            generation_time = end_time - start_time
            
            return {
                "thinking": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}",
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.",
                "full_response": f"Error: {e}",
                "token_stats": {
                    "input_tokens": input_tokens,
                    "output_tokens": 0,
                    "total_tokens": input_tokens,
                    "tokens_per_second": 0.0,
                    "generation_time": generation_time
                }
            }

def print_token_stats(token_stats: Dict[str, Any]):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤"""
    print(f"\n{Back.MAGENTA}{Fore.WHITE} TOKEN STATISTICS {Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}‚îå{'‚îÄ'*60}‚îê")
    print(f"{Fore.MAGENTA}‚îÇ {Fore.CYAN}üì• –í—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:  {token_stats['input_tokens']:>8} {Fore.MAGENTA}                ‚îÇ")
    print(f"{Fore.MAGENTA}‚îÇ {Fore.CYAN}üì§ –í—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {token_stats['output_tokens']:>8} {Fore.MAGENTA}                ‚îÇ")
    print(f"{Fore.MAGENTA}‚îÇ {Fore.CYAN}üìä –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤:    {token_stats['total_tokens']:>8} {Fore.MAGENTA}                ‚îÇ")
    print(f"{Fore.MAGENTA}‚îÇ {Fore.CYAN}‚ö° –°–∫–æ—Ä–æ—Å—Ç—å:         {token_stats['tokens_per_second']:>8.2f} —Ç/—Å–µ–∫ {Fore.MAGENTA}      ‚îÇ")
    print(f"{Fore.MAGENTA}‚îÇ {Fore.CYAN}‚è±Ô∏è  –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {token_stats['generation_time']:>8.2f} —Å–µ–∫ {Fore.MAGENTA}       ‚îÇ")
    print(f"{Fore.MAGENTA}‚îî{'‚îÄ'*60}‚îò")

def print_separator():
    """–ü–µ—á–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è"""
    print(f"{Fore.CYAN}{'='*80}")

def print_thinking(thinking_text: str):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ thinking —Å–µ–∫—Ü–∏–∏"""
    print(f"\n{Back.BLUE}{Fore.WHITE} THINKING PROCESS {Style.RESET_ALL}")
    print(f"{Fore.BLUE}‚îå{'‚îÄ'*78}‚îê")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    lines = thinking_text.split('\n')
    for line in lines:
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫–∏
        if len(line) > 76:
            words = line.split(' ')
            current_line = ""
            for word in words:
                if len(current_line + word) > 76:
                    print(f"{Fore.BLUE}‚îÇ {Fore.CYAN}{current_line:<76}{Fore.BLUE} ‚îÇ")
                    current_line = word + " "
                else:
                    current_line += word + " "
            if current_line.strip():
                print(f"{Fore.BLUE}‚îÇ {Fore.CYAN}{current_line.strip():<76}{Fore.BLUE} ‚îÇ")
        else:
            print(f"{Fore.BLUE}‚îÇ {Fore.CYAN}{line:<76}{Fore.BLUE} ‚îÇ")
    
    print(f"{Fore.BLUE}‚îî{'‚îÄ'*78}‚îò")

def print_answer(answer_text: str):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
    print(f"\n{Back.GREEN}{Fore.WHITE} FINAL ANSWER {Style.RESET_ALL}")
    print(f"{Fore.GREEN}{answer_text}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —á–∞—Ç–∞"""
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description='LLM Chat Interface with Thinking Mode and Audio Support')
    parser.add_argument('--model', '-m', type=str, help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ GGUF')
    parser.add_argument('--test', '-t', action='store_true', help='–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏)')
    parser.add_argument('--no-audio', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å –∞—É–¥–∏–æ —Ñ—É–Ω–∫—Ü–∏–∏')
    parser.add_argument('--no-stt', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏')
    parser.add_argument('--no-tts', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏')
    args = parser.parse_args()
    
    try:
        print(f"{Fore.MAGENTA}ü§ñ LLM Chat Interface with Thinking Mode & Audio")
        print(f"{Fore.MAGENTA}–ú–æ–¥–µ–ª—å: Huihui-Qwen3-4B-Thinking")
        print_separator()
        
        if args.test:
            print(f"{Fore.YELLOW}üß™ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è - –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è")
            print(f"{Fore.GREEN}‚úì –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
            print(f"{Fore.BLUE}–î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–µ–∑ —Ñ–ª–∞–≥–∞ --test")
            return
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ –º–æ–¥—É–ª—è
        audio_handler = None
        if not args.no_audio:
            try:
                audio_handler = AudioHandler()
                if args.no_stt:
                    audio_handler.toggle_stt(False)
                if args.no_tts:
                    audio_handler.toggle_tts(False)
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è  –ê—É–¥–∏–æ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                print(f"{Fore.CYAN}üí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞—É–¥–∏–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pyaudio speechrecognition pyttsx3")
                audio_handler = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vision –º–æ–¥—É–ª—è
        vision_handler = None
        try:
            vision_handler = VisionHandler()
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è  Vision –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            print(f"{Fore.CYAN}üí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è vision —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install opencv-python Pillow numpy")
            vision_handler = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –º–æ–¥–µ–ª–∏
        config = LLMConfig(args.model)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config.print_config()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        if not Path(config.model_path).exists():
            print(f"{Fore.RED}‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {config.model_path}")
            print(f"{Fore.YELLOW}üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä --model –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏:")
            print(f"{Fore.CYAN}   python main.py --model \"C:/path/to/your/model.gguf\"")
            print(f"{Fore.YELLOW}üí° –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --test –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:")
            print(f"{Fore.CYAN}   python main.py --test")
            return
        
        llm = ThinkingLLM(config)
        
        print_separator()
        print(f"{Fore.GREEN}‚úì –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        
        # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
        print(f"{Fore.YELLOW}–ö–æ–º–∞–Ω–¥—ã:")
        print(f"{Fore.CYAN}  'quit' –∏–ª–∏ 'exit' - –≤—ã—Ö–æ–¥")
        print(f"{Fore.CYAN}  'clear' - –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞")
        print(f"{Fore.CYAN}  'history' - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞")
        if audio_handler and audio_handler.is_speech_recognition_available():
            print(f"{Fore.CYAN}  'listen' - —Ä–µ–∂–∏–º –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞")
        if audio_handler:
            print(f"{Fore.CYAN}  'audio on/off' - –≤–∫–ª—é—á–∏—Ç—å/–æ—Ç–∫–ª—é—á–∏—Ç—å –∞—É–¥–∏–æ")
        if vision_handler and vision_handler.is_camera_available():
            print(f"{Fore.CYAN}  'vision start/stop' - –∑–∞–ø—É—Å–∫/–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–º–µ—Ä—ã")
            print(f"{Fore.CYAN}  'look' - –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞")
            print(f"{Fore.CYAN}  'cameras' - –ø–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞–º–µ—Ä—ã")
            print(f"{Fore.CYAN}  'camera <id>' - –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∫–∞–º–µ—Ä—É –ø–æ ID")
            print(f"{Fore.CYAN}  'refresh cameras' - –æ–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞–º–µ—Ä")
        print_separator()
        
        while True:
            try:
                # –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                user_input = input(f"\n{Fore.YELLOW}üë§ –í–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'listen' –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞): {Style.RESET_ALL}")
                
                if user_input.lower().strip() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
                    print(f"{Fore.BLUE}–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã")
                    break
                
                if user_input.lower().strip() == 'clear':
                    llm.clear_history()
                    continue
                
                if user_input.lower().strip() == 'history':
                    print(f"\n{Fore.MAGENTA}üìö –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:")
                    for i, msg in enumerate(llm.conversation_history):
                        if msg['role'] == 'system':
                            continue
                        role_emoji = "üë§" if msg['role'] == 'user' else "ü§ñ"
                        content_preview = msg['content'][:100] + "..." if len(msg['content']) > 100 else msg['content']
                        print(f"{Fore.CYAN}{i}. {role_emoji} {msg['role']}: {content_preview}")
                    print(f"{Fore.BLUE}{llm.get_history_summary()}")
                    continue
                
                # –ö–æ–º–∞–Ω–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞—É–¥–∏–æ
                if audio_handler and user_input.lower().strip() == 'audio on':
                    audio_handler.toggle_stt(True)
                    audio_handler.toggle_tts(True)
                    continue
                
                if audio_handler and user_input.lower().strip() == 'audio off':
                    audio_handler.toggle_stt(False)
                    audio_handler.toggle_tts(False)
                    continue
                
                # –ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥
                if user_input.lower().strip() == 'listen':
                    if audio_handler and audio_handler.is_speech_recognition_available():
                        print(f"{Fore.CYAN}üé§ –ü–µ—Ä–µ—Ö–æ–¥ –≤ —Ä–µ–∂–∏–º –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞...")
                        voice_input = audio_handler.listen_for_question()
                        if voice_input:
                            print(f"{Fore.GREEN}üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {voice_input}")
                            user_input = voice_input
                        else:
                            print(f"{Fore.YELLOW}‚ùì –ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑")
                            continue
                    else:
                        print(f"{Fore.RED}‚ùå –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
                        continue
                
                # –ö–æ–º–∞–Ω–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è vision
                if vision_handler and user_input.lower().strip() == 'vision start':
                    if vision_handler.start_real_time_analysis():
                        print(f"{Fore.GREEN}üìπ –ê–Ω–∞–ª–∏–∑ –∫–∞–º–µ—Ä—ã –∑–∞–ø—É—â–µ–Ω")
                    else:
                        print(f"{Fore.RED}‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∫–∞–º–µ—Ä—ã")
                    continue
                
                if vision_handler and user_input.lower().strip() == 'vision stop':
                    vision_handler.stop_real_time_analysis()
                    print(f"{Fore.YELLOW}‚èπÔ∏è  –ê–Ω–∞–ª–∏–∑ –∫–∞–º–µ—Ä—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                    continue
                
                # –ö–æ–º–∞–Ω–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–º–µ—Ä–∞–º–∏
                if vision_handler and user_input.lower().strip() == 'cameras':
                    available_cameras = vision_handler.list_cameras()
                    current_camera = vision_handler.get_current_camera()
                    print(f"{Fore.CYAN}üìπ –¢–µ–∫—É—â–∞—è –∫–∞–º–µ—Ä–∞: {current_camera}")
                    if available_cameras:
                        print(f"{Fore.GREEN}‚úì –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞–º–µ—Ä—ã: {available_cameras}")
                    else:
                        print(f"{Fore.RED}‚ùå –ö–∞–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    continue
                
                if vision_handler and user_input.lower().strip().startswith('camera '):
                    try:
                        camera_id = int(user_input.split()[1])
                        if vision_handler.switch_camera(camera_id):
                            print(f"{Fore.GREEN}‚úì –ü–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –Ω–∞ –∫–∞–º–µ—Ä—É {camera_id}")
                        else:
                            print(f"{Fore.RED}‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∫–∞–º–µ—Ä—É {camera_id}")
                    except (ValueError, IndexError):
                        print(f"{Fore.RED}‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: camera <id>")
                    continue
                
                if vision_handler and user_input.lower().strip() == 'refresh cameras':
                    available_cameras = vision_handler.refresh_cameras()
                    if available_cameras:
                        print(f"{Fore.GREEN}‚úì –û–±–Ω–æ–≤–ª–µ–Ω–æ! –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞–º–µ—Ä—ã: {available_cameras}")
                    else:
                        print(f"{Fore.RED}‚ùå –ö–∞–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    continue
                
                if user_input.lower().strip() == 'look':
                    if vision_handler and vision_handler.is_camera_available():
                        print(f"{Fore.CYAN}üëÅÔ∏è  –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞...")
                        analysis = vision_handler.analyze_single_frame("–û–ø–∏—à–∏—Ç–µ —á—Ç–æ –≤—ã –≤–∏–¥–∏—Ç–µ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
                        print(f"{Fore.GREEN}üîç –ê–Ω–∞–ª–∏–∑: {analysis}")
                        continue
                    else:
                        print(f"{Fore.RED}‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                        continue
                
                if not user_input.strip():
                    continue
                
                print_separator()
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                response = llm.generate_response(user_input)
                
                # –í—ã–≤–æ–¥ thinking –ø—Ä–æ—Ü–µ—Å—Å–∞
                if response["thinking"]:
                    print_thinking(response["thinking"])
                
                # –í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                if response["answer"]:
                    print_answer(response["answer"])
                else:
                    print(f"{Fore.RED}–û—à–∏–±–∫–∞: –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
                
                # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
                if "token_stats" in response:
                    print_token_stats(response["token_stats"])
                
                # –û–∑–≤—É—á–∏–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
                if audio_handler and audio_handler.is_text_to_speech_available() and response["answer"]:
                    print(f"\n{Fore.CYAN}üîä –û–∑–≤—É—á–∏–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞...")
                    audio_handler.speak_answer(response["answer"], play_async=True)
                
                print_separator()
                
            except KeyboardInterrupt:
                print(f"\n{Fore.BLUE}–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã")
                break
            except Exception as e:
                print(f"{Fore.RED}–û—à–∏–±–∫–∞: {e}")
                continue
        
        # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        if vision_handler:
            vision_handler.cleanup()
                
    except Exception as e:
        print(f"{Fore.RED}–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
    finally:
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º globals() –≤–º–µ—Å—Ç–æ locals() –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            if 'vision_handler' in globals() and 'vision_handler' in locals():
                vision_handler_obj = locals().get('vision_handler')
                if vision_handler_obj:
                    vision_handler_obj.cleanup()
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ

if __name__ == "__main__":
    main()